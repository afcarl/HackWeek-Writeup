\section*{Audience and Participant Selection}

Hack weeks differ from traditional conferences or summer schools in that knowledge transfer occurs across many levels of seniority, disciplinary boundaries, and novelty of the topics discussed.
In addition, a substantial amount of hack week content is generated during the event itself, requiring active participation from attendees.
Therefore in order to maximize learning outcomes and collaborative exchanges, it is crucial that the participant selection process be carried out with considerable care.

In our experience, a participant group that is diverse across categories of minority status, geographical origin, gender, discipline and career track helps to ensure we meet these objectives.
To achieve this diversity, we advocate for a selection process that is as quantitative and transparent as possible, enabling participants to hold organizers accountable for their selection decisions.

Participant selection is an intrinsically difficult process that is fraught with personal and structural biases~\cite[e.g.][]{sunstein2015wiser}. Traditional selection processes rely heavily on internal heuristics of reviewers, including using unrelated characteristics including name~\cite{bertrand2004} or gender~\cite{mossracusin2012} as a cipher for suitability. On the other hand, while purely mechanistic procedures may to be able to mitigate some reviewer biases, this may come at a cost of baking in structural inequalities. A blind selection purely on from figure of merit will likely be biased if some groups of applicants have been systematically disadvantaged in certain ways: for example, if workshop organizers require a minimum level of programming experience, this could disadvantage candidates who have had fewer opportunities to learn programming due to structural inequalities. 

Research particularly in the hiring literature has shown that cohort selection is most effective and unbiased when selection procedures are as quantitative as possible~\cite{sunstein2015wiser}. 
In practice, there are different approaches to counteract intrinsic human biases and provide transparency. 
Because human decision makers tend to be swayed by unrelated characteristics including name~\cite{bertrand2004} or gender~\cite{mossracusin2012}, an initial merit selection blinded to demographic characteristics can be an effective way to counteract certain biases. A merit selection could then be performed via scores given independently by members of the organizing committee based on a set of pre-defined, explicit selection criteria. 

However, blinding in the initial merit selection step is most effective at counteracting biases when the hack week targets a broad population, where the initial selection does not directly depend on skill level (as has been true e.g.\ for Astro Hack Week; see also SM, Section 2.1). 
For workshops where merit selection is intrinsically tied to skill levels, organizers should be mindful that blinding to demographics can be counterproductive if it excludes participants who might have had less exposure to certain technologies or fewer opportunities to learn certain skills. Additionally, blinding has been found to have negative effects on diversity for committees that have already had a strong commitment to diversity, because they often correct for structural inequalities by considering demographic variables during merit selection~\cite{behaghel2015unintended}.
In this case, it may be beneficial to construct selection criteria that explicitly consider diversity and inclusivity (as Neuro Hack Week has done; see also SM, Section 2.1). 
Because systemic biases likely also enter at the application stage (where underrepresented groups may be less likely to apply) organizers should consider oversampling traditionally disenfranchised groups compared to the population of applicants. 

Since human decision makers also routinely overestimate their ability to forecast future performance of a candidate and underestimate intrinsic variance during selection procedures, ~\cite{highhouse2008} it may be beneficial to set a fairly tolerant threshold for acceptance based on merit criteria, and select the cohort from this meritous pool via e.g.\ via an algorithm, imposing outside constraints on the selection based on the goals of the workshop. 

One solution to the latter problem is implemented in the software \textit{entrofy}\footnote{\url{http://github.com/dhuppenkothen/entrofy}}). 
The algorithm aims to find a group of participants that together match as closely as possible a requested distribution on specified dimensions (e.g., career stage, geographic location, etc.), to meet pre-set fractions set by the organizers.
It is worth noting that this algorithm is vulnerable toward biases in two ways: firstly, humans will set the target fractions for any category of interest.  
Any human biases involved in setting these target fractions will be perpetuated in the selection procedure. 
Secondly, perhaps more obviously, the algorithm can only act on information that has been collected.
Biased participant sets may still result from selection procedures that fail to include crucial categories. 

No matter the selection procedures used, we encourage organizers to critically examine their cohort selection, experiment with new approaches, and routinely evaluate their procedure. For example, comparing demographic characteristics of the selected versus non-selected groups can unveil unintended biases during the merit-selection base and thus allow adjustments in the procedure to mitigate or fully remove these effects.
