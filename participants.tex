\section*{Audience and Participant Selection}
Hack weeks differ from traditional conferences or summer schools in that knowledge transfer occurs across many levels of seniority, disciplinary boundaries, and novelty of the topics discussed.
In addition, a substantial amount of hack week content is generated during the event itself, requiring active participation from participants.
Therefore in order to maximize learning outcomes and collaborative exchanges, it is crucial that the participant selection process be carried out with considerable care.

In our experience, a participant group that is diverse across categories of minority status, geographical origin, gender, discipline and career track helps to ensure we meet these objectives.
To achieve this diversity, we advocate for a selection process that is as quantitative and transparent as possible, enabling participants to hold organizers accountable for their selection decisions.

Participant selection is an intrinsically difficult process that is fraught with personal and structural biases~\cite[e.g.][]{sunstein2015wiser}. Traditional selection processes rely heavily on internal heuristics of reviewers, including using unrelated characteristics including name~\cite{bertrand2004} or gender~\cite{mossracusin2012} as a cipher for suitability. On the other hand, while purely mechanistic procedures may to be able to mitigate some reviewer biases, this may come at a cost of baking in structural inequalities. A blind selection purely on merit may be flawed if some groups of applicants have been systematically disadvantaged in certain ways: for example, 

Research particularly in the hiring literature has shown that cohort selection is most effective and unbiased when selection procedures are as quantitative as possible~\cite{sunstein2015wiser}. 
In practice, there are different approaches to counteract intrinsic human biases and provide transparency. 
Because human decision makers tend to be swayed by unrelated characteristics including name~\cite{bertrand2004} or gender~\cite{mossracusin2012}, an initial merit selection blinded to demographic characteristics can be an effective way to counteract certain biases. A merit selection could then perform via scores given independently by members of the organizing committee based on a set of pre-defined, explicit selection criteria. 
Since human decision makers also routinely overestimate their ability to forecast future performance of a candidate, ~\cite{highhouse2008} it may be beneficial to subsequently set a fairly tolerant threshold for acceptance, and select the cohort via e.g.\ via an algorithm, imposing outside constraints on the selection based on the goals of the workshop. 

One solution to the latter problem is implemented in the software \textit{entrofy}\footnote{\url{http://github.com/dhuppenkothen/entrofy}}). 
The algorithm aims to find a group of participants that together match as closely as possible a requested distribution on specified dimensions (e.g., career stage, geographic location, etc.), to meet pre-set fractions set by the organizers.
It is worth noting that this algorithm is vulnerable toward biases in two ways: firstly, humans will set the target fractions for any category of interest.  
Any human biases involved in setting these target fractions will be perpetuated in the selection procedure. 
Secondly, perhaps more obviously, the algorithm can only act on information that has been collected.
Biased participant sets may still result from selection procedures that fail to include crucial categories. 

Blinding in the initial merit selection step is most effective at counteracting biases when the hack week targets a broad population. 
For more targeted workshops, organizers should be mindful that blinding to demographics might be counterproductive~\cite{behaghel2015unintended} if it excludes participants who might have had less exposure to certain technologies or fewer opportunities to learn certain skills. 
In this case, it may be beneficial to include demographic characteristics in the first stage. 
Because systemic biases likely also enter at the application stage (where underrepresented groups may be less likely to apply) organizers should consider oversampling traditionally disenfranchised groups compared to the population of applicants. 

No matter the selection procedures used, we encourage organizers to critically examine their cohort selection, experiment with new approaches, and routinely evaluate their procedure (see also the Supplementary materials for more details on how the previous hack weeks approached this problem). 